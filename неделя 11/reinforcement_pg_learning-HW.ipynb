{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is python : True\n",
      "Device : cuda\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.nn.init import kaiming_normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "import tqdm\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "print(\"Is python : {}\".format(is_ipython))\n",
    "\n",
    "#plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device : {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNet(nn.Module):\n",
    "    # плейсхолдер для определения модели\n",
    "    # не забудьте, что выход policy-gradients модели - вероятности действий\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        #x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        #x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        #x = F.relu(self.conv3(x))\n",
    "        x = self.head(x.view(x.size(0), -1))\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вход\n",
    "\n",
    "\n",
    "Вытаскиваем картинку из среды. Используем бонусы из ``torchvision``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "# This is based on the code from gym.\n",
    "screen_width = 600\n",
    "\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))  # транспонирование в порядок торча (СHW)\n",
    "    # Убираем верх и низ экрана\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Убираем края экрана, чтобы получить картинку с центрированной тележкой\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Конвертируем в торч тензор\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Ресайзим и добавляем батч размерность\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5000\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE=10\n",
    "\n",
    "policy_net = PGNet().to(device)\n",
    "\n",
    "# можно попробовать\n",
    "optimizer = optim.SGD(policy_net.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        #print(\"Return by policy\")\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        #print(\"Return random\")\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    \n",
    "# метод для десконтирования reward'ов\n",
    "def discount_rewards(rewards):\n",
    "    R = 0\n",
    "    \n",
    "    for r in rewards[::-1]:\n",
    "        R = r + GAMMA * R\n",
    "    \n",
    "    return(R)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# метод для оптимизации модели\n",
    "def optimize_model(log_probas, rewards):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тренируем модель\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9099e06eb66a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlast_100_ep_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy_net' is not defined"
     ]
    }
   ],
   "source": [
    "num_episodes = 500 # суммарное кол-во эпизодов\n",
    "ep_per_epoch = 5 # кол-во эпизодов м/у обучением\n",
    "\n",
    "log_probas = []\n",
    "rewards = []\n",
    "\n",
    "policy_net.train()\n",
    "last_100_ep_duration = 0\n",
    "\n",
    "for i_episode in tqdm.tqdm(range(num_episodes)):\n",
    "    \n",
    "    # Ура! Выйграли этот энвайронмент!\n",
    "    if last_100_ep_duration >= 195:\n",
    "        break\n",
    "    \n",
    "    # Инициализация среды\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    \n",
    "    for t in count():\n",
    "    \n",
    "        # тут какая-то магия, при которой мы получаем вер-ти очередного действия\n",
    "        \n",
    "        \n",
    "        # Выбрать и выполнить новое действие\n",
    "        pass\n",
    "\n",
    "        # Получаем новое состояние\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Переходим в новое состояние\n",
    "        state = next_state\n",
    "            \n",
    "        if done:\n",
    "            # Оптимайзимся\n",
    "            if (i_episode + 1) % ep_per_epoch == 0:\n",
    "                optimize_model(log_probas, rewards)\n",
    "\n",
    "            episode_durations.append(t + 1)\n",
    "            last_100_ep_durations = np.mean(episode_durations[-100:])\n",
    "            print(\"Mean last 100 ep durations : {}\".format(last_100_ep_durations))\n",
    "            plot_durations()\n",
    "            break\n",
    "            \n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "#plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (env.spec.id, step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "env.reset()\n",
    "last_screen = get_screen()\n",
    "current_screen = get_screen()\n",
    "state = current_screen - last_screen\n",
    "#print(type(state))\n",
    "total_reward = 0\n",
    "\n",
    "policy_net.eval()\n",
    "\n",
    "for i in range(1000):\n",
    "    # Выбрать и выполнить нове действие\n",
    "    m = Categorical(policy_net.forward(state))\n",
    "    action = m.sample()\n",
    "    _,reward,done,_ = env.step(action.item())\n",
    "    total_reward += reward\n",
    "    # Получаем новое состояние\n",
    "    last_screen = current_screen\n",
    "    current_screen = get_screen()\n",
    "    if not done:\n",
    "        next_state = current_screen - last_screen\n",
    "    else:\n",
    "        next_state = None\n",
    "        break\n",
    "    state = next_state\n",
    "    show_state(env, i)\n",
    "print(f\"Total reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
