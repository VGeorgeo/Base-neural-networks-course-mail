{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word = 'ololoasdasddqweqw123456789'\n",
    "word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.06047058105469\n",
      "Clip gradient :  7.437175003988109\n",
      "58.754878997802734\n",
      "Clip gradient :  6.7159120324765595\n",
      "45.875450134277344\n",
      "Clip gradient :  8.084153846317244\n",
      "42.543846130371094\n",
      "Clip gradient :  21.162578326745187\n",
      "31.38055992126465\n",
      "Clip gradient :  5.894581662237843\n",
      "25.658594131469727\n",
      "Clip gradient :  5.735804175426364\n",
      "22.810142517089844\n",
      "Clip gradient :  5.3126005054326315\n",
      "20.45781898498535\n",
      "Clip gradient :  6.864212647914329\n",
      "17.558513641357422\n",
      "Clip gradient :  8.02025901609971\n",
      "17.57586669921875\n",
      "Clip gradient :  9.743617080932442\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t oasdasdasdasdasdasdasdasda\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-612b341693d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Prediction:\\t'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpredword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово\n",
    "Сохранить ноутбук с предсказанием и пройденным assert и прислать на почту a.murashev@corp.mail.ru\n",
    "c темой:\n",
    "\n",
    "\n",
    "[МФТИ\\_2019\\_1] ДЗ №8 ФИО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "#word = 'ololoasdasddqweqw123456789'\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(LSTM, self).__init__()  \n",
    "        self.x2hidden_input = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden_input = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2hidden_forget = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden_forget = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2hidden_output = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden_output = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2cand_cell_state = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.cand_cell_state = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.activation  = nn.Tanh()\n",
    "        \n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        \n",
    "        candidate = self.activation(self.x2cand_cell_state(x) + self.cand_cell_state(prev_hidden))\n",
    "        input_gate = self.sigmoid(self.x2hidden_output(x) + self.hidden_input(prev_hidden))\n",
    "        forget_gate = self.sigmoid(self.x2hidden_forget(x) + self.hidden_forget(prev_hidden))\n",
    "        output_gate = self.sigmoid(self.x2hidden_output(x) + self.hidden_output(prev_hidden))\n",
    "        \n",
    "        cell_state = forget_gate*prev_hidden + input_gate*candidate\n",
    "        hidden = output_gate*self.activation(cell_state)\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = LSTM(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 800\n",
    "optim     = SGD(rnn.parameters(), lr = 0.0035, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.8020248413086\n",
      "Clip gradient :  3.5050267784786002\n",
      "72.12062072753906\n",
      "Clip gradient :  3.243286563539369\n",
      "71.06831359863281\n",
      "Clip gradient :  2.7817593259763482\n",
      "70.08607482910156\n",
      "Clip gradient :  2.2690146762808205\n",
      "69.29735565185547\n",
      "Clip gradient :  1.7985476193503322\n",
      "68.69398498535156\n",
      "Clip gradient :  1.4677829324651221\n",
      "68.20228576660156\n",
      "Clip gradient :  1.373507118804837\n",
      "67.71556091308594\n",
      "Clip gradient :  1.524025675979223\n",
      "67.138427734375\n",
      "Clip gradient :  1.8150381609253683\n",
      "66.42091369628906\n",
      "Clip gradient :  2.1356243233334915\n",
      "65.55620574951172\n",
      "Clip gradient :  2.422937258518237\n",
      "64.56068420410156\n",
      "Clip gradient :  2.658914121781265\n",
      "63.455894470214844\n",
      "Clip gradient :  2.8499409435834053\n",
      "62.25980758666992\n",
      "Clip gradient :  3.0069306059569136\n",
      "60.987300872802734\n",
      "Clip gradient :  3.1309170176923415\n",
      "59.654930114746094\n",
      "Clip gradient :  3.218194544090391\n",
      "58.2811393737793\n",
      "Clip gradient :  3.27142256625065\n",
      "56.88232421875\n",
      "Clip gradient :  3.300041300229083\n",
      "55.4701042175293\n",
      "Clip gradient :  3.3136378259582373\n",
      "54.05189514160156\n",
      "Clip gradient :  3.3164780485457737\n",
      "52.63380432128906\n",
      "Clip gradient :  3.306880893527979\n",
      "51.22319793701172\n",
      "Clip gradient :  3.2812485322790064\n",
      "49.82876968383789\n",
      "Clip gradient :  3.2401280955916256\n",
      "48.457645416259766\n",
      "Clip gradient :  3.189837601598414\n",
      "47.11310958862305\n",
      "Clip gradient :  3.137271231751618\n",
      "45.7953987121582\n",
      "Clip gradient :  3.084851125783907\n",
      "44.50448989868164\n",
      "Clip gradient :  3.03089736585401\n",
      "43.2419548034668\n",
      "Clip gradient :  2.9717426543235352\n",
      "42.0113525390625\n",
      "Clip gradient :  2.904093141925799\n",
      "40.81721496582031\n",
      "Clip gradient :  2.8276146241843225\n",
      "39.66322708129883\n",
      "Clip gradient :  2.7446886230298095\n",
      "38.55146026611328\n",
      "Clip gradient :  2.658711442342162\n",
      "37.482452392578125\n",
      "Clip gradient :  2.5730515930418685\n",
      "36.45523452758789\n",
      "Clip gradient :  2.4906401753731338\n",
      "35.46746063232422\n",
      "Clip gradient :  2.4146273180871787\n",
      "34.515228271484375\n",
      "Clip gradient :  2.3486258722363957\n",
      "33.59309768676758\n",
      "Clip gradient :  2.2952177692889695\n",
      "32.69535446166992\n",
      "Clip gradient :  2.251067983148186\n",
      "31.819377899169922\n",
      "Clip gradient :  2.2040560194379237\n",
      "30.967588424682617\n",
      "Clip gradient :  2.146482803376428\n",
      "30.143301010131836\n",
      "Clip gradient :  2.0851223410851976\n",
      "29.34662437438965\n",
      "Clip gradient :  2.0263359549936397\n",
      "28.57569694519043\n",
      "Clip gradient :  1.9713489254426206\n",
      "27.82880401611328\n",
      "Clip gradient :  1.9191778835168456\n",
      "27.10492515563965\n",
      "Clip gradient :  1.8683084990968342\n",
      "26.403650283813477\n",
      "Clip gradient :  1.8175802175301854\n",
      "25.724796295166016\n",
      "Clip gradient :  1.7670314516197294\n",
      "25.067977905273438\n",
      "Clip gradient :  1.7172705380919941\n",
      "24.43254280090332\n",
      "Clip gradient :  1.6685690417048464\n",
      "23.817842483520508\n",
      "Clip gradient :  1.6208862656125467\n",
      "23.223323822021484\n",
      "Clip gradient :  1.5741866174023857\n",
      "22.648447036743164\n",
      "Clip gradient :  1.528496050056497\n",
      "22.092681884765625\n",
      "Clip gradient :  1.483831594307042\n",
      "21.55547523498535\n",
      "Clip gradient :  1.4401773895195344\n",
      "21.03628921508789\n",
      "Clip gradient :  1.3974893587090154\n",
      "20.534605026245117\n",
      "Clip gradient :  1.3557199112123521\n",
      "20.049922943115234\n",
      "Clip gradient :  1.3148387841082372\n",
      "19.581764221191406\n",
      "Clip gradient :  1.2748596504581535\n",
      "19.129650115966797\n",
      "Clip gradient :  1.2358462617229677\n",
      "18.693065643310547\n",
      "Clip gradient :  1.1979078890813442\n",
      "18.27145767211914\n",
      "Clip gradient :  1.161184612006823\n",
      "17.864233016967773\n",
      "Clip gradient :  1.1258236534965278\n",
      "17.470731735229492\n",
      "Clip gradient :  1.0919565621840865\n",
      "17.09027862548828\n",
      "Clip gradient :  1.0596859479304166\n",
      "16.72215461730957\n",
      "Clip gradient :  1.0290774115654862\n",
      "16.365642547607422\n",
      "Clip gradient :  1.0001663425244365\n",
      "16.021278381347656\n",
      "Clip gradient :  0.9730699955911316\n",
      "15.691896438598633\n",
      "Clip gradient :  0.9480670677617097\n",
      "15.378011703491211\n",
      "Clip gradient :  0.9252555229152689\n",
      "15.078612327575684\n",
      "Clip gradient :  0.9046491584386804\n",
      "14.792228698730469\n",
      "Clip gradient :  0.886281370587975\n",
      "14.517274856567383\n",
      "Clip gradient :  0.8702154636609863\n",
      "14.25216007232666\n",
      "Clip gradient :  0.8564566105243352\n",
      "13.995376586914062\n",
      "Clip gradient :  0.8447207068022046\n",
      "13.74569320678711\n",
      "Clip gradient :  0.8342554699556014\n",
      "13.502339363098145\n",
      "Clip gradient :  0.824293936400857\n",
      "13.264877319335938\n",
      "Clip gradient :  0.814637472521182\n",
      "13.03293228149414\n",
      "Clip gradient :  0.8054407509376966\n",
      "12.806114196777344\n",
      "Clip gradient :  0.796975397289808\n",
      "12.58397388458252\n",
      "Clip gradient :  0.789306866060899\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden_input.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden_input.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = LSTM(in_size=ds.vec_size, hidden_size=13, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 500\n",
    "optim     = SGD(rnn.parameters(), lr = 0.0035, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.98316192626953\n",
      "Clip gradient :  3.727144909833264\n",
      "71.2453384399414\n",
      "Clip gradient :  3.403254159518163\n",
      "70.13980102539062\n",
      "Clip gradient :  2.842741248359161\n",
      "69.15157318115234\n",
      "Clip gradient :  2.2304626126031395\n",
      "68.40304565429688\n",
      "Clip gradient :  1.7081155563420243\n",
      "67.8493423461914\n",
      "Clip gradient :  1.4599906199666144\n",
      "67.35659790039062\n",
      "Clip gradient :  1.510708692693682\n",
      "66.80661010742188\n",
      "Clip gradient :  1.7129873487323994\n",
      "66.14325714111328\n",
      "Clip gradient :  1.9958630798225947\n",
      "65.33881378173828\n",
      "Clip gradient :  2.3270818854257675\n",
      "64.37190246582031\n",
      "Clip gradient :  2.7023517440101648\n",
      "63.21278762817383\n",
      "Clip gradient :  3.125429528645227\n",
      "61.82717514038086\n",
      "Clip gradient :  3.578986768352163\n",
      "60.19051742553711\n",
      "Clip gradient :  4.0361508741756875\n",
      "58.298744201660156\n",
      "Clip gradient :  4.442337809950821\n",
      "56.182865142822266\n",
      "Clip gradient :  4.747804037796753\n",
      "53.897342681884766\n",
      "Clip gradient :  4.966897478544474\n",
      "51.4775505065918\n",
      "Clip gradient :  5.163303438324523\n",
      "48.949684143066406\n",
      "Clip gradient :  5.334889276109821\n",
      "46.337562561035156\n",
      "Clip gradient :  5.480517672307531\n",
      "43.65607833862305\n",
      "Clip gradient :  5.614733445920568\n",
      "40.905460357666016\n",
      "Clip gradient :  5.766994417324931\n",
      "38.104217529296875\n",
      "Clip gradient :  5.744390542362128\n",
      "35.415977478027344\n",
      "Clip gradient :  5.767968628125708\n",
      "32.855018615722656\n",
      "Clip gradient :  5.505534912467702\n",
      "30.42357063293457\n",
      "Clip gradient :  5.2360555688536214\n",
      "28.127389907836914\n",
      "Clip gradient :  4.921435114351538\n",
      "25.959983825683594\n",
      "Clip gradient :  4.615078707706431\n",
      "23.90012550354004\n",
      "Clip gradient :  4.443937669493351\n",
      "21.928638458251953\n",
      "Clip gradient :  4.159915109115153\n",
      "20.0782470703125\n",
      "Clip gradient :  3.9304300043075617\n",
      "18.356525421142578\n",
      "Clip gradient :  3.688271132994507\n",
      "16.764419555664062\n",
      "Clip gradient :  3.4605132095223414\n",
      "15.297943115234375\n",
      "Clip gradient :  3.235740122875828\n",
      "13.951007843017578\n",
      "Clip gradient :  3.0213480686497003\n",
      "12.715680122375488\n",
      "Clip gradient :  2.8184745174433457\n",
      "11.58298110961914\n",
      "Clip gradient :  2.628835813623287\n",
      "10.543561935424805\n",
      "Clip gradient :  2.453077159590072\n",
      "9.588294982910156\n",
      "Clip gradient :  2.291006966524246\n",
      "8.708673477172852\n",
      "Clip gradient :  2.1421423651889104\n",
      "7.896883010864258\n",
      "Clip gradient :  2.006121840128643\n",
      "7.145646572113037\n",
      "Clip gradient :  1.882790654223787\n",
      "6.448184490203857\n",
      "Clip gradient :  1.7713420486249132\n",
      "5.798680305480957\n",
      "Clip gradient :  1.6686007492507984\n",
      "5.193398952484131\n",
      "Clip gradient :  1.5682541516483577\n",
      "4.631510257720947\n",
      "Clip gradient :  1.464267800948035\n",
      "4.114038467407227\n",
      "Clip gradient :  1.3552075319460009\n",
      "3.641906261444092\n",
      "Clip gradient :  1.2428686446063084\n",
      "3.2150654792785645\n",
      "Clip gradient :  1.130542901420053\n",
      "2.8321313858032227\n",
      "Clip gradient :  1.021920991028372\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden_input.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden_input.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию GRU и обучить предсказывать слово\n",
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(GRU, self).__init__()  \n",
    "        self.x2update = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.update = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2reset = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.reset = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2hidden_out = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden_out = nn.Linear(in_features=hidden_size, out_features=hidden_size)   \n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.activation  = nn.Tanh()\n",
    "        \n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        \n",
    "        update_gate = self.sigmoid(self.x2update(x) + self.update(prev_hidden))\n",
    "        reset_gate = self.sigmoid(self.x2reset(x) + self.reset(prev_hidden))\n",
    "        hidden_output = self.activation(self.x2hidden_out(x) + self.hidden_out(reset_gate*prev_hidden))\n",
    "\n",
    "        hidden = (1-update_gate)*hidden_output + update_gate*prev_hidden\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = GRU(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 2000\n",
    "optim     = SGD(rnn.parameters(), lr = 0.055, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.87786102294922\n",
      "Clip gradient :  5.188306800588145\n",
      "66.8559799194336\n",
      "Clip gradient :  2.3696214817023638\n",
      "59.3044319152832\n",
      "Clip gradient :  3.9175002259330007\n",
      "45.96525955200195\n",
      "Clip gradient :  3.9976920765850466\n",
      "34.796142578125\n",
      "Clip gradient :  2.6504255873884772\n",
      "28.036130905151367\n",
      "Clip gradient :  1.815157995049056\n",
      "23.365081787109375\n",
      "Clip gradient :  1.479896514426469\n",
      "19.481264114379883\n",
      "Clip gradient :  1.3148412708741959\n",
      "16.372228622436523\n",
      "Clip gradient :  1.249680026927593\n",
      "14.247832298278809\n",
      "Clip gradient :  1.859410242095364\n",
      "13.171234130859375\n",
      "Clip gradient :  6.359223858177733\n",
      "13.93499755859375\n",
      "Clip gradient :  14.66719131366566\n",
      "13.289627075195312\n",
      "Clip gradient :  11.064466598866426\n",
      "13.131752014160156\n",
      "Clip gradient :  10.336749869044029\n",
      "12.711416244506836\n",
      "Clip gradient :  8.969250076095255\n",
      "12.726506233215332\n",
      "Clip gradient :  10.8622171662519\n",
      "12.077552795410156\n",
      "Clip gradient :  7.379678292885951\n",
      "11.842845916748047\n",
      "Clip gradient :  5.032360325973545\n",
      "11.098884582519531\n",
      "Clip gradient :  4.53456184474012\n",
      "10.995638847351074\n",
      "Clip gradient :  5.83842192204187\n",
      "10.402628898620605\n",
      "Clip gradient :  3.142818817967409\n",
      "9.825471878051758\n",
      "Clip gradient :  2.3382570146872106\n",
      "9.441536903381348\n",
      "Clip gradient :  1.7148317168229765\n",
      "9.103981971740723\n",
      "Clip gradient :  1.23534825038591\n",
      "8.782888412475586\n",
      "Clip gradient :  3.0527288920022655\n",
      "8.596471786499023\n",
      "Clip gradient :  4.1142463202105635\n",
      "8.509063720703125\n",
      "Clip gradient :  3.8377488502868755\n",
      "8.780492782592773\n",
      "Clip gradient :  8.546802122982266\n",
      "9.229433059692383\n",
      "Clip gradient :  8.40943026680523\n",
      "10.992271423339844\n",
      "Clip gradient :  9.360545757837487\n",
      "9.403748512268066\n",
      "Clip gradient :  5.2801184531201235\n",
      "9.246604919433594\n",
      "Clip gradient :  7.762515617138676\n",
      "9.922080039978027\n",
      "Clip gradient :  7.946521859835311\n",
      "8.355291366577148\n",
      "Clip gradient :  2.6524333294842486\n",
      "7.967103958129883\n",
      "Clip gradient :  2.431885604450163\n",
      "7.515564918518066\n",
      "Clip gradient :  1.18153396109609\n",
      "7.241649627685547\n",
      "Clip gradient :  1.9174560643303096\n",
      "7.283616065979004\n",
      "Clip gradient :  6.9148226469067495\n",
      "7.931833267211914\n",
      "Clip gradient :  14.997684222121995\n",
      "9.023841857910156\n",
      "Clip gradient :  18.501947821633983\n",
      "7.2333574295043945\n",
      "Clip gradient :  5.006121544996917\n",
      "9.205738067626953\n",
      "Clip gradient :  26.291754320555395\n",
      "7.8317670822143555\n",
      "Clip gradient :  13.339133575759773\n",
      "8.609527587890625\n",
      "Clip gradient :  22.825925547038203\n",
      "7.316035747528076\n",
      "Clip gradient :  6.791578545253853\n",
      "8.55792236328125\n",
      "Clip gradient :  23.200355203421907\n",
      "7.032113552093506\n",
      "Clip gradient :  1.5982550976414551\n",
      "6.799280166625977\n",
      "Clip gradient :  1.3354525193792322\n",
      "6.584111213684082\n",
      "Clip gradient :  0.9046395416084423\n",
      "6.38007116317749\n",
      "Clip gradient :  0.32691888692339416\n",
      "6.224790573120117\n",
      "Clip gradient :  0.8060250075962856\n",
      "6.141096115112305\n",
      "Clip gradient :  1.8842189771113504\n",
      "6.080395698547363\n",
      "Clip gradient :  2.10019818847872\n",
      "5.9851579666137695\n",
      "Clip gradient :  0.583986130335189\n",
      "5.940216541290283\n",
      "Clip gradient :  2.3887471499823096\n",
      "5.892111778259277\n",
      "Clip gradient :  2.60957297411976\n",
      "5.801848888397217\n",
      "Clip gradient :  1.562760113069957\n",
      "5.726500511169434\n",
      "Clip gradient :  0.6169861490489796\n",
      "5.662845134735107\n",
      "Clip gradient :  1.6881738437409564\n",
      "5.61440896987915\n",
      "Clip gradient :  2.018106666614885\n",
      "5.61203145980835\n",
      "Clip gradient :  2.205143012519552\n",
      "5.529345989227295\n",
      "Clip gradient :  0.38737883236735177\n",
      "5.490405559539795\n",
      "Clip gradient :  1.6721157186967381\n",
      "5.448429584503174\n",
      "Clip gradient :  2.019109741702212\n",
      "5.7824625968933105\n",
      "Clip gradient :  8.432396211124912\n",
      "6.521419048309326\n",
      "Clip gradient :  17.42008613936491\n",
      "6.202023506164551\n",
      "Clip gradient :  11.606078744278635\n",
      "5.831582546234131\n",
      "Clip gradient :  5.781802794842485\n",
      "6.991247177124023\n",
      "Clip gradient :  41.93356323658059\n",
      "6.160982608795166\n",
      "Clip gradient :  13.299816275085064\n",
      "6.454525947570801\n",
      "Clip gradient :  26.848942078156636\n",
      "6.229676246643066\n",
      "Clip gradient :  18.002485318151663\n",
      "5.66146993637085\n",
      "Clip gradient :  7.058458333279865\n",
      "6.113675117492676\n",
      "Clip gradient :  12.36146936550616\n",
      "5.513067245483398\n",
      "Clip gradient :  2.379240851056044\n",
      "5.351663589477539\n",
      "Clip gradient :  0.8638373804799384\n",
      "5.289969444274902\n",
      "Clip gradient :  1.7006560804470083\n",
      "5.255436897277832\n",
      "Clip gradient :  2.250154933698916\n",
      "5.214624404907227\n",
      "Clip gradient :  2.0075372060605745\n",
      "5.151676654815674\n",
      "Clip gradient :  1.3976899518931647\n",
      "5.03848123550415\n",
      "Clip gradient :  1.27172792514521\n",
      "4.767250061035156\n",
      "Clip gradient :  0.36517881631746\n",
      "4.486767292022705\n",
      "Clip gradient :  0.33282588793805246\n",
      "4.264944553375244\n",
      "Clip gradient :  0.25823649645313934\n",
      "4.1214823722839355\n",
      "Clip gradient :  0.7561631918604086\n",
      "4.022269248962402\n",
      "Clip gradient :  0.9299544375311924\n",
      "3.942448139190674\n",
      "Clip gradient :  0.7849702550737492\n",
      "3.873513698577881\n",
      "Clip gradient :  0.14439090368226384\n",
      "3.8188905715942383\n",
      "Clip gradient :  0.13367959189426423\n",
      "3.771512031555176\n",
      "Clip gradient :  0.12546793905508297\n",
      "3.729604721069336\n",
      "Clip gradient :  0.11866701352238979\n",
      "3.691987991333008\n",
      "Clip gradient :  0.11279051299366367\n",
      "3.6578426361083984\n",
      "Clip gradient :  0.10781371450976282\n",
      "3.626587390899658\n",
      "Clip gradient :  0.10342033641664712\n",
      "3.597778797149658\n",
      "Clip gradient :  0.0994846126768589\n",
      "3.5710906982421875\n",
      "Clip gradient :  0.09591868311333171\n",
      "3.5462608337402344\n",
      "Clip gradient :  0.09266202481170796\n",
      "3.5230636596679688\n",
      "Clip gradient :  0.08966423837332053\n",
      "3.5013251304626465\n",
      "Clip gradient :  0.08689251206752285\n",
      "3.4808993339538574\n",
      "Clip gradient :  0.08431960524040456\n",
      "3.46165132522583\n",
      "Clip gradient :  0.08192316216119147\n",
      "3.4434680938720703\n",
      "Clip gradient :  0.07968511390296776\n",
      "3.4262566566467285\n",
      "Clip gradient :  0.07759180430465247\n",
      "3.4099268913269043\n",
      "Clip gradient :  0.07563226669260527\n",
      "3.3944053649902344\n",
      "Clip gradient :  0.0737975818808012\n",
      "3.3796191215515137\n",
      "Clip gradient :  0.07207972553106458\n",
      "3.365508556365967\n",
      "Clip gradient :  0.07047402630588932\n",
      "3.3520078659057617\n",
      "Clip gradient :  0.06897638464281072\n",
      "3.339064121246338\n",
      "Clip gradient :  0.06758791867136298\n",
      "3.32663631439209\n",
      "Clip gradient :  0.06631768769790732\n",
      "3.3146538734436035\n",
      "Clip gradient :  0.06517404872935927\n",
      "3.3030710220336914\n",
      "Clip gradient :  0.06418488090407755\n",
      "3.2918167114257812\n",
      "Clip gradient :  0.063396604528503\n",
      "3.2808189392089844\n",
      "Clip gradient :  0.0629074583080609\n",
      "3.26993989944458\n",
      "Clip gradient :  0.06290874805755567\n",
      "3.2589774131774902\n",
      "Clip gradient :  0.06387136516158405\n",
      "3.247481346130371\n",
      "Clip gradient :  0.06710804446655481\n",
      "3.234179973602295\n",
      "Clip gradient :  0.07737291963984072\n",
      "3.213296890258789\n",
      "Clip gradient :  0.11952499562534528\n",
      "3.1209678649902344\n",
      "Clip gradient :  0.38823492511564356\n",
      "2.817729949951172\n",
      "Clip gradient :  10.262192582704435\n",
      "9.39753246307373\n",
      "Clip gradient :  92.87423916610098\n",
      "7.615378379821777\n",
      "Clip gradient :  38.78322708651164\n",
      "3.2861104011535645\n",
      "Clip gradient :  4.266761047026869\n",
      "3.0084333419799805\n",
      "Clip gradient :  3.212480196527254\n",
      "2.619659900665283\n",
      "Clip gradient :  1.6151340704894792\n",
      "2.583657741546631\n",
      "Clip gradient :  11.218344181716013\n",
      "3.557919502258301\n",
      "Clip gradient :  18.49193999369537\n",
      "4.053958415985107\n",
      "Clip gradient :  14.831292186708362\n",
      "3.0119338035583496\n",
      "Clip gradient :  4.807211838431481\n",
      "2.779426097869873\n",
      "Clip gradient :  2.9040534656484924\n",
      "2.6098146438598633\n",
      "Clip gradient :  4.1294307738422535\n",
      "2.7806248664855957\n",
      "Clip gradient :  6.6385482338670965\n",
      "3.0334672927856445\n",
      "Clip gradient :  8.859351551073544\n",
      "2.756319999694824\n",
      "Clip gradient :  5.532062064505641\n",
      "3.0001935958862305\n",
      "Clip gradient :  3.8765741677361607\n",
      "2.5343103408813477\n",
      "Clip gradient :  1.728820797389259\n",
      "2.3351216316223145\n",
      "Clip gradient :  3.302998717163642\n",
      "2.8163461685180664\n",
      "Clip gradient :  30.706842370164114\n",
      "10.598005294799805\n",
      "Clip gradient :  62.29370054312981\n",
      "7.728189468383789\n",
      "Clip gradient :  62.51476532632797\n",
      "2.5068249702453613\n",
      "Clip gradient :  4.494885690234896\n",
      "3.7800002098083496\n",
      "Clip gradient :  4.2528002278876755\n",
      "3.1620936393737793\n",
      "Clip gradient :  1.6533295044286773\n",
      "2.4978675842285156\n",
      "Clip gradient :  3.9533738658698616\n",
      "2.2787022590637207\n",
      "Clip gradient :  1.3839891879652724\n",
      "2.147099018096924\n",
      "Clip gradient :  0.6104722267591267\n",
      "2.0288991928100586\n",
      "Clip gradient :  0.3060042154659011\n",
      "1.9690971374511719\n",
      "Clip gradient :  0.27325872510856325\n",
      "1.9399094581604004\n",
      "Clip gradient :  1.2302776672906217\n",
      "1.906449317932129\n",
      "Clip gradient :  0.4057685027444081\n",
      "1.8925232887268066\n",
      "Clip gradient :  1.1213931757075912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8747239112854004\n",
      "Clip gradient :  0.837718973237058\n",
      "1.8741135597229004\n",
      "Clip gradient :  1.5661424772895927\n",
      "1.860508918762207\n",
      "Clip gradient :  0.9843192916705618\n",
      "1.8396868705749512\n",
      "Clip gradient :  0.9062164748161158\n",
      "1.8240880966186523\n",
      "Clip gradient :  0.3383635385555431\n",
      "1.8148870468139648\n",
      "Clip gradient :  0.5255805001039033\n",
      "1.9219202995300293\n",
      "Clip gradient :  4.917594403450506\n",
      "15.296342849731445\n",
      "Clip gradient :  16.37532452107007\n",
      "11.35098648071289\n",
      "Clip gradient :  22.604804377127415\n",
      "6.064920425415039\n",
      "Clip gradient :  26.177884248183464\n",
      "2.2441534996032715\n",
      "Clip gradient :  4.931797524986466\n",
      "2.0749826431274414\n",
      "Clip gradient :  2.772492253330128\n",
      "1.91550874710083\n",
      "Clip gradient :  1.1039791231374494\n",
      "1.8414382934570312\n",
      "Clip gradient :  0.3983959595148351\n",
      "1.8157544136047363\n",
      "Clip gradient :  0.11649638560944876\n",
      "1.8000574111938477\n",
      "Clip gradient :  0.0784632101285742\n",
      "1.7878165245056152\n",
      "Clip gradient :  0.06295103557053104\n",
      "1.7774934768676758\n",
      "Clip gradient :  0.058432695912305405\n",
      "1.7684297561645508\n",
      "Clip gradient :  0.05513026906151679\n",
      "1.7602853775024414\n",
      "Clip gradient :  0.052581899344505585\n",
      "1.752852439880371\n",
      "Clip gradient :  0.0504891896541797\n",
      "1.7459759712219238\n",
      "Clip gradient :  0.048702923351320655\n",
      "1.7395691871643066\n",
      "Clip gradient :  0.04714941280884638\n",
      "1.7335495948791504\n",
      "Clip gradient :  0.045772130450059845\n",
      "1.7278704643249512\n",
      "Clip gradient :  0.04453741240637473\n",
      "1.7224946022033691\n",
      "Clip gradient :  0.04342310501390563\n",
      "1.7173728942871094\n",
      "Clip gradient :  0.04240430139299254\n",
      "1.7124896049499512\n",
      "Clip gradient :  0.041469414155736656\n",
      "1.7078146934509277\n",
      "Clip gradient :  0.04060507638665764\n",
      "1.7033252716064453\n",
      "Clip gradient :  0.039801544424767556\n",
      "1.6990132331848145\n",
      "Clip gradient :  0.0390517971110185\n",
      "1.694864273071289\n",
      "Clip gradient :  0.038349880375707496\n",
      "1.6908583641052246\n",
      "Clip gradient :  0.037688230235359124\n",
      "1.68699312210083\n",
      "Clip gradient :  0.037064680166553864\n",
      "1.683244228363037\n",
      "Clip gradient :  0.036470631585671286\n",
      "1.6796140670776367\n",
      "Clip gradient :  0.035906051278216326\n",
      "1.676095962524414\n",
      "Clip gradient :  0.03536857717160212\n",
      "1.6726865768432617\n",
      "Clip gradient :  0.034860047535399036\n",
      "1.6693682670593262\n",
      "Clip gradient :  0.03436464216357685\n",
      "1.6661505699157715\n",
      "Clip gradient :  0.033895168804630456\n",
      "1.6630096435546875\n",
      "Clip gradient :  0.03344330328100563\n",
      "1.6599540710449219\n",
      "Clip gradient :  0.033008732829208055\n",
      "1.6569838523864746\n",
      "Clip gradient :  0.032586752206842956\n",
      "1.654083251953125\n",
      "Clip gradient :  0.03217966116313044\n",
      "1.6512565612792969\n",
      "Clip gradient :  0.0317888594566741\n",
      "1.6484947204589844\n",
      "Clip gradient :  0.03140931084423305\n",
      "1.6457977294921875\n",
      "Clip gradient :  0.03114065815776012\n",
      "1.6431665420532227\n",
      "Clip gradient :  0.03098150465034577\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden_out.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden_out.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = GRU(in_size=ds.vec_size, hidden_size=13, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 200\n",
    "optim     = SGD(rnn.parameters(), lr = 0.055, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.39786529541016\n",
      "Clip gradient :  4.987064269980409\n",
      "64.61773681640625\n",
      "Clip gradient :  3.62434131760271\n",
      "50.63269805908203\n",
      "Clip gradient :  6.13333817603911\n",
      "31.830459594726562\n",
      "Clip gradient :  4.831189811514762\n",
      "18.17308807373047\n",
      "Clip gradient :  3.6051484314701923\n",
      "8.588116645812988\n",
      "Clip gradient :  2.425961084059931\n",
      "3.3659610748291016\n",
      "Clip gradient :  1.2547837448155956\n",
      "1.1851539611816406\n",
      "Clip gradient :  0.5287296474615661\n",
      "0.6112232208251953\n",
      "Clip gradient :  0.2834386216981405\n",
      "0.4169178009033203\n",
      "Clip gradient :  0.1969429686538978\n",
      "0.3203458786010742\n",
      "Clip gradient :  0.15348125933504336\n",
      "0.26098155975341797\n",
      "Clip gradient :  0.12651812753757427\n",
      "0.22021865844726562\n",
      "Clip gradient :  0.10784004374298634\n",
      "0.19034099578857422\n",
      "Clip gradient :  0.09402846249622998\n",
      "0.1674633026123047\n",
      "Clip gradient :  0.08335494893390355\n",
      "0.14938068389892578\n",
      "Clip gradient :  0.07484124154386727\n",
      "0.1347370147705078\n",
      "Clip gradient :  0.06788754412814359\n",
      "0.1226348876953125\n",
      "Clip gradient :  0.06209801719526665\n",
      "0.11247539520263672\n",
      "Clip gradient :  0.05720879914479251\n",
      "0.10382461547851562\n",
      "Clip gradient :  0.05302500233212064\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden_out.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden_out.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
